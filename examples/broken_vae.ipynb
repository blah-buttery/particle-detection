{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a939581-bfae-41bb-882c-8564d254e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add particle_detection to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581a66dd-501f-471b-93c2-d0c683554387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from particle_detection.autoencoder.model import create_autoencoder\n",
    "#from particle_detection.data.dataset import ImageDataset, get_transforms\n",
    "from particle_detection.data.dataset import get_transforms\n",
    "from particle_detection.data.dataset import create_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5958890a-1c5e-42ff-92b6-eee1574e8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from particle_detection.autoencoder.model import create_autoencoder\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_files, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Simple Dataset class for loading images.\n",
    "\n",
    "        :param image_files: List of image file names.\n",
    "        :param data_dir: Path to the directory containing images.\n",
    "        :param transform: Transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_files = image_files\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Normalize and convert the image to RGB format.\n",
    "\n",
    "    :param image: Input PIL image.\n",
    "    :return: Preprocessed PIL image.\n",
    "    \"\"\"\n",
    "    sample = np.array(image)\n",
    "    sample = cv2.normalize(sample, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    processed_image = Image.fromarray(sample).convert('RGB')\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def get_transforms(image_size=(224, 224), is_train=True):\n",
    "    \"\"\"\n",
    "    Simple function to get transformations for training or testing.\n",
    "\n",
    "    :param image_size: Tuple of desired image dimensions (height, width).\n",
    "    :param is_train: Whether to include augmentation (True for training, False for testing).\n",
    "    :return: A torchvision.transforms.Compose object.\n",
    "    \"\"\"\n",
    "    transform_list = [\n",
    "        transforms.Lambda(preprocess_image),  # Integrate preprocessing here\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "\n",
    "    if is_train:\n",
    "        # Add augmentations only for training\n",
    "        transform_list.extend([\n",
    "            transforms.RandomRotation(degrees=(-30, 30)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        ])\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "def load_image_file_paths(data_dir, extensions=(\".tif\", \".TIF\")):\n",
    "    \"\"\"\n",
    "    List all image file paths in the directory with the given extensions.\n",
    "\n",
    "    :param data_dir: Path to the directory containing images.\n",
    "    :param extensions: Tuple of allowed image file extensions.\n",
    "    :return: List of image file names.\n",
    "    \"\"\"\n",
    "    return [f for f in os.listdir(data_dir) if f.endswith(extensions)]\n",
    "\n",
    "\n",
    "def split_dataset(image_files, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split image file names into training and testing sets.\n",
    "\n",
    "    :param image_files: List of image file names.\n",
    "    :param test_size: Proportion of the dataset to use for testing.\n",
    "    :param random_seed: Random seed for reproducibility.\n",
    "    :return: Tuple of (train_files, test_files).\n",
    "    \"\"\"\n",
    "    return train_test_split(image_files, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "\n",
    "def create_dataloaders(data_dir, transform, batch_size, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoader objects for training and testing.\n",
    "\n",
    "    :param data_dir: Path to the directory containing images.\n",
    "    :param transform: Transform to apply to the images.\n",
    "    :param batch_size: Batch size for DataLoaders.\n",
    "    :param test_size: Proportion of the dataset to use for testing.\n",
    "    :return: Tuple of (train_loader, test_loader).\n",
    "    \"\"\"\n",
    "    # Step 1: Load all image file paths\n",
    "    image_files = load_image_file_paths(data_dir)\n",
    "\n",
    "    # Step 2: Split into train and test sets\n",
    "    train_files, test_files = split_dataset(image_files, test_size=test_size)\n",
    "\n",
    "    # Step 3: Create Dataset objects\n",
    "    train_dataset = ImageDataset(train_files, data_dir, transform=transform)\n",
    "    test_dataset = ImageDataset(test_files, data_dir, transform=transform)\n",
    "\n",
    "    # Step 4: Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83429c33-b99f-49ba-80f7-66168b4f0da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of training batches: 1\n",
      "[INFO] Number of testing batches: 1\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/blah_m4/Desktop/nanoparticle/images\"\n",
    "transform = get_transforms(image_size=(224, 224), is_train=True)\n",
    "batch_size = 16\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(data_dir, transform, batch_size)\n",
    "\n",
    "print(f\"[INFO] Number of training batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e289114c-2535-42f2-8b92-883e96cdbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images in train_loader:\n",
    "    print(images.shape)  # Prints the shape of the first batch of images\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e20c1b-52f3-44e7-8d96-9247ff287816",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m---> 34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3873\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n\u001b[1;32m   3862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3863\u001b[0m         mse_loss,\n\u001b[1;32m   3864\u001b[0m         (\u001b[38;5;28minput\u001b[39m, target, weight),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3870\u001b[0m         weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m   3871\u001b[0m     )\n\u001b[0;32m-> 3873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[1;32m   3874\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3875\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3876\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3878\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3879\u001b[0m     )\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# Get transforms\n",
    "transform = get_transforms(image_size=(1024, 1024), is_train=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, test_loader = create_dataloaders(data_dir, transform, batch_size)\n",
    "\n",
    "# Validate batch size\n",
    "if batch_size > len(train_loader.dataset):\n",
    "    print(f\"[WARNING] Batch size ({batch_size}) exceeds dataset size ({len(train_loader.dataset)}). Adjusting batch size.\")\n",
    "    batch_size = len(train_loader.dataset)\n",
    "\n",
    "model = create_autoencoder()\n",
    "model = nn.DataParallel(model)\n",
    "#model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e826ada8-286f-4723-a6b6-1a9398b8622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 1024 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 512 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 256 -> 128\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(256 * 128 * 128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 128 * 128, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 256 * 128 * 128)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 256 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, in_channels, kernel_size=4, stride=2, padding=1),  # 512 -> 1024\n",
    "            nn.Sigmoid()  # Output normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std * epsilon\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_flat = encoded.view(encoded.size(0), -1)\n",
    "        \n",
    "        # Latent variables\n",
    "        mu = self.fc_mu(encoded_flat)\n",
    "        logvar = self.fc_logvar(encoded_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        decoded_flat = self.fc_decoder(z)\n",
    "        decoded = decoded_flat.view(-1, 256, 128, 128)\n",
    "        x_reconstructed = self.decoder(decoded)\n",
    "\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Loss function for VAE\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss as the sum of reconstruction loss and KL divergence.\n",
    "    :param recon_x: Reconstructed input.\n",
    "    :param x: Original input.\n",
    "    :param mu: Mean of the latent distribution.\n",
    "    :param logvar: Log variance of the latent distribution.\n",
    "    \"\"\"\n",
    "    #recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "def create_vae():\n",
    "    \"\"\"\n",
    "    Creates and returns a Variational Autoencoder (VAE) instance.\n",
    "    \"\"\"\n",
    "    return VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c5978af-c08e-44c8-9c30-8e09df4950df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized version \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 1024 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),          # 512 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),         # 256 -> 128\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((8, 8))  # Reduce spatial size to 8x8\n",
    "        )\n",
    "        \n",
    "        # Latent space\n",
    "        self.flattened_dim = 256 * 8 * 8  # Adjust based on encoder output\n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decoder = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 256 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, in_channels, kernel_size=4, stride=2, padding=1),  # 512 -> 1024\n",
    "            nn.Sigmoid()  # Output normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_flat = encoded.view(encoded.size(0), -1)\n",
    "        \n",
    "        # Latent variables\n",
    "        mu = self.fc_mu(encoded_flat)\n",
    "        logvar = self.fc_logvar(encoded_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        decoded_flat = self.fc_decoder(z)\n",
    "        decoded = decoded_flat.view(-1, 256, 8, 8)  # Adjust based on encoder output\n",
    "        x_reconstructed = self.decoder(decoded)\n",
    "\n",
    "        return x_reconstructed, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee68b055-9dca-4284-beb5-1f7f4ca7c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 1024 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),          # 512 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),         # 256 -> 128\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space\n",
    "        self.flattened_dim = 256 * 128 * 128  # Adjusted for input size 1024x1024\n",
    "        self.fc_mu = nn.Linear(self.flattened_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decoder = nn.Linear(latent_dim, self.flattened_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 256 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, in_channels, kernel_size=4, stride=2, padding=1),  # 512 -> 1024\n",
    "            nn.Sigmoid()  # Normalize output to [0, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std * epsilon\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")  # Debug input shape\n",
    "        \n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        print(f\"Encoded shape: {encoded.shape}\")  # Debug encoded shape\n",
    "        encoded_flat = encoded.view(encoded.size(0), -1)\n",
    "        \n",
    "        # Latent variables\n",
    "        mu = self.fc_mu(encoded_flat)\n",
    "        logvar = self.fc_logvar(encoded_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        decoded_flat = self.fc_decoder(z)\n",
    "        decoded = decoded_flat.view(-1, 256, 128, 128)  # Reshape to match encoder output\n",
    "        print(f\"Decoded (before upsampling) shape: {decoded.shape}\")  # Debug decoded shape\n",
    "        x_reconstructed = self.decoder(decoded)\n",
    "        print(f\"Reconstructed shape: {x_reconstructed.shape}\")  # Debug reconstructed shape\n",
    "\n",
    "        # Ensure reconstruction matches input size\n",
    "        x_reconstructed = F.interpolate(x_reconstructed, size=x.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        print(f\"Final reconstructed shape (after interpolation): {x_reconstructed.shape}\")  # Debug final shape\n",
    "\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Loss function for VAE\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss as the sum of reconstruction loss and KL divergence.\n",
    "    :param recon_x: Reconstructed input.\n",
    "    :param x: Original input.\n",
    "    :param mu: Mean of the latent distribution.\n",
    "    :param logvar: Log variance of the latent distribution.\n",
    "    \"\"\"\n",
    "    # Reconstruction loss\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "def create_vae():\n",
    "    \"\"\"\n",
    "    Creates and returns a Variational Autoencoder (VAE) instance.\n",
    "    \"\"\"\n",
    "    return VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8cf8b18-68a1-4662-b823-bf49de78b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Create VAE model\\nmodel = create_vae()\\n#model.to(device)\\nmodel = nn.DataParallel(model).to(device)\\n#model.to(device)\\n\\n# Define loss function and optimizer\\ncriterion = nn.MSELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 20\n",
    "batch_size = 1\n",
    "dataset_dir = \"/home/blah-buttery/nanoparticles/images/normal\"  # gpu workstation image location\n",
    "#dataset_dir = \"/Users/blah_m4/Desktop/nanoparticle/images\" # macbook image location\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# Get transforms\n",
    "transform = get_transforms(image_size=(1024, 1024), is_train=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, test_loader = create_dataloaders(dataset_dir, transform, batch_size)\n",
    "\n",
    "# Validate batch size\n",
    "if batch_size > len(train_loader.dataset):\n",
    "    print(f\"[WARNING] Batch size ({batch_size}) exceeds dataset size ({len(train_loader.dataset)}). Adjusting batch size.\")\n",
    "    batch_size = len(train_loader.dataset)\n",
    "'''\n",
    "# Create VAE model\n",
    "model = create_vae()\n",
    "#model.to(device)\n",
    "model = nn.DataParallel(model).to(device)\n",
    "#model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5e803a3-7f84-4610-9d15-c9635d3340cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 1024, 1024])\n",
      "Encoded shape: torch.Size([1, 256, 128, 128])\n",
      "Decoded (before upsampling) shape: torch.Size([1, 256, 128, 128])\n",
      "Reconstructed shape: torch.Size([1, 3, 1024, 1024])\n",
      "Final reconstructed shape (after interpolation): torch.Size([1, 3, 1024, 1024])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.57 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 25\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m recon_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/particle_detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/particle_detection/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/particle_detection/.venv/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    231\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     adam(\n\u001b[1;32m    245\u001b[0m         params_with_grad,\n\u001b[1;32m    246\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/particle_detection/.venv/lib/python3.10/site-packages/torch/optim/adam.py:174\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    164\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    166\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    178\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    179\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    180\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 22.57 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "num_epochs = 50\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    recon_loss_total = 0.0\n",
    "    kl_div_total = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, mu, logvar = model(batch)\n",
    "        \n",
    "        # Compute losses\n",
    "        recon_loss = nn.functional.mse_loss(reconstructed, batch, reduction='sum')  # Reconstruction loss\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence\n",
    "        loss = recon_loss + kl_div\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        recon_loss_total += recon_loss.item()\n",
    "        kl_div_total += kl_div.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Reconstruction Loss: {recon_loss_total / len(train_loader):.4f}, \"\n",
    "          f\"KL Divergence: {kl_div_total / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"[INFO] Testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762646f-4a10-4a65-9b13-2550a1b22873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop without extra information\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, mu, logvar = model(batch)\n",
    "        loss = vae_loss(reconstructed, batch, mu, logvar)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"[INFO] Testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c15a111-802b-4346-b784-c63f2b4925c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 5            |        cudaMalloc retries: 5         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  20878 MiB |  20878 MiB |  36886 MiB |  16008 MiB |\n",
      "|       from large pool |  20875 MiB |  20875 MiB |  36879 MiB |  16004 MiB |\n",
      "|       from small pool |      3 MiB |      3 MiB |      7 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  20878 MiB |  20878 MiB |  36886 MiB |  16008 MiB |\n",
      "|       from large pool |  20875 MiB |  20875 MiB |  36879 MiB |  16004 MiB |\n",
      "|       from small pool |      3 MiB |      3 MiB |      7 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  20878 MiB |  20878 MiB |  36886 MiB |  16008 MiB |\n",
      "|       from large pool |  20875 MiB |  20875 MiB |  36879 MiB |  16004 MiB |\n",
      "|       from small pool |      3 MiB |      3 MiB |      7 MiB |      4 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  22652 MiB |  22652 MiB |  22652 MiB |      0 B   |\n",
      "|       from large pool |  22648 MiB |  22648 MiB |  22648 MiB |      0 B   |\n",
      "|       from small pool |      4 MiB |      4 MiB |      4 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   1773 MiB |   2050 MiB |   7624 MiB |   5851 MiB |\n",
      "|       from large pool |   1772 MiB |   2049 MiB |   7616 MiB |   5843 MiB |\n",
      "|       from small pool |      0 MiB |      1 MiB |      8 MiB |      8 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      73    |      73    |     182    |     109    |\n",
      "|       from large pool |      26    |      26    |      81    |      55    |\n",
      "|       from small pool |      47    |      47    |     101    |      54    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      73    |      73    |     182    |     109    |\n",
      "|       from large pool |      26    |      26    |      81    |      55    |\n",
      "|       from small pool |      47    |      47    |     101    |      54    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      15    |      15    |      15    |       0    |\n",
      "|       from large pool |      13    |      13    |      13    |       0    |\n",
      "|       from small pool |       2    |       2    |       2    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |      10    |      48    |      43    |\n",
      "|       from large pool |       4    |       5    |      18    |      14    |\n",
      "|       from small pool |       1    |       6    |      30    |      29    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcbb62e6-807a-4673-8336-b11eec0c8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_vae(model, train_loader, optimizer, num_epochs, accumulation_steps=4, device=\"cuda\"):\n",
    "    # Initialize mixed precision GradScaler\n",
    "    scaler = GradScaler()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            optimizer.zero_grad(set_to_none=True)  # Use set_to_none=True for better memory optimization\n",
    "            with autocast():\n",
    "                reconstructed, mu, logvar = model(batch)\n",
    "                loss = vae_loss(reconstructed, batch, mu, logvar)\n",
    "                loss = loss / accumulation_steps  # Scale loss by accumulation steps\n",
    "\n",
    "            # Backward pass with mixed precision scaling\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Perform optimizer step after accumulating gradients\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)  # Reset gradients\n",
    "\n",
    "            # Accumulate training loss\n",
    "            train_loss += loss.item() * accumulation_steps  # Rescale to original loss\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"[INFO] Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abef4e2e-e3be-4fa7-ab61-afcfe0cc4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def train_vae(model, train_loader, optimizer, num_epochs, accumulation_steps=2, device=\"cuda\"):\n",
    "    scaler = GradScaler()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Clear GPU cache to avoid fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Mixed precision training\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                reconstructed, mu, logvar = model(batch)\n",
    "                loss = vae_loss(reconstructed, batch, mu, logvar)\n",
    "                loss = loss / accumulation_steps  # Scale loss for gradient accumulation\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Perform optimizer step after gradient accumulation\n",
    "            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        avg_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"[INFO] Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304f4a96-6aa6-4205-b775-212626f4d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8734/218816850.py:5: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/tmp/ipykernel_8734/218816850.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(device_type=\"cuda\"):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "autocast.__init__() got an unexpected keyword argument 'device_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(vae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mtrain_vae\u001b[0;34m(model, train_loader, optimizer, num_epochs, accumulation_steps, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Mixed precision training\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mautocast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     21\u001b[0m     reconstructed, mu, logvar \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m vae_loss(reconstructed, batch, mu, logvar)\n",
      "File \u001b[0;32m~/particle_detection/.venv/lib/python3.10/site-packages/typing_extensions.py:2853\u001b[0m, in \u001b[0;36mdeprecated.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2850\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(arg)\n\u001b[1;32m   2851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2852\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39mcategory, stacklevel\u001b[38;5;241m=\u001b[39mstacklevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: autocast.__init__() got an unexpected keyword argument 'device_type'"
     ]
    }
   ],
   "source": [
    "# Define model, optimizer, and dataloader\n",
    "vae = VAE(in_channels=3, latent_dim=128)\n",
    "vae = nn.DataParallel(vae).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "#train_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train_vae(model=vae, train_loader=train_loader, optimizer=optimizer, num_epochs=10, accumulation_steps=4, device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc64b6-4f27-41a1-bc68-9f6c4a02cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
