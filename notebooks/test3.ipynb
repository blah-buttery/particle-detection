{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a939581-bfae-41bb-882c-8564d254e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add particle_detection to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5958890a-1c5e-42ff-92b6-eee1574e8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from particle_detection.autoencoder.model import create_autoencoder\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_files, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Simple Dataset class for loading images.\n",
    "\n",
    "        :param image_files: List of image file names.\n",
    "        :param data_dir: Path to the directory containing images.\n",
    "        :param transform: Transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.image_files = image_files\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_dir, self.image_files[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Normalize and convert the image to RGB format.\n",
    "\n",
    "    :param image: Input PIL image.\n",
    "    :return: Preprocessed PIL image.\n",
    "    \"\"\"\n",
    "    sample = np.array(image)\n",
    "    sample = cv2.normalize(sample, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    processed_image = Image.fromarray(sample).convert('RGB')\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def get_transforms(image_size=(224, 224), is_train=True):\n",
    "    \"\"\"\n",
    "    Simple function to get transformations for training or testing.\n",
    "\n",
    "    :param image_size: Tuple of desired image dimensions (height, width).\n",
    "    :param is_train: Whether to include augmentation (True for training, False for testing).\n",
    "    :return: A torchvision.transforms.Compose object.\n",
    "    \"\"\"\n",
    "    transform_list = [\n",
    "        transforms.Lambda(preprocess_image),  # Integrate preprocessing here\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "\n",
    "    if is_train:\n",
    "        # Add augmentations only for training\n",
    "        transform_list.extend([\n",
    "            transforms.RandomRotation(degrees=(-30, 30)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "        ])\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "def load_image_file_paths(data_dir, extensions=(\".tif\", \".TIF\")):\n",
    "    \"\"\"\n",
    "    List all image file paths in the directory with the given extensions.\n",
    "\n",
    "    :param data_dir: Path to the directory containing images.\n",
    "    :param extensions: Tuple of allowed image file extensions.\n",
    "    :return: List of image file names.\n",
    "    \"\"\"\n",
    "    return [f for f in os.listdir(data_dir) if f.endswith(extensions)]\n",
    "\n",
    "\n",
    "def split_dataset(image_files, test_size=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split image file names into training and testing sets.\n",
    "\n",
    "    :param image_files: List of image file names.\n",
    "    :param test_size: Proportion of the dataset to use for testing.\n",
    "    :param random_seed: Random seed for reproducibility.\n",
    "    :return: Tuple of (train_files, test_files).\n",
    "    \"\"\"\n",
    "    return train_test_split(image_files, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "\n",
    "def create_dataloaders(data_dir, transform, batch_size, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoader objects for training and testing.\n",
    "\n",
    "    :param data_dir: Path to the directory containing images.\n",
    "    :param transform: Transform to apply to the images.\n",
    "    :param batch_size: Batch size for DataLoaders.\n",
    "    :param test_size: Proportion of the dataset to use for testing.\n",
    "    :return: Tuple of (train_loader, test_loader).\n",
    "    \"\"\"\n",
    "    # Step 1: Load all image file paths\n",
    "    image_files = load_image_file_paths(data_dir)\n",
    "\n",
    "    # Step 2: Split into train and test sets\n",
    "    train_files, test_files = split_dataset(image_files, test_size=test_size)\n",
    "\n",
    "    # Step 3: Create Dataset objects\n",
    "    train_dataset = ImageDataset(train_files, data_dir, transform=transform)\n",
    "    test_dataset = ImageDataset(test_files, data_dir, transform=transform)\n",
    "\n",
    "    # Step 4: Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83429c33-b99f-49ba-80f7-66168b4f0da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of training batches: 1\n",
      "[INFO] Number of testing batches: 1\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/blah_m4/Desktop/nanoparticle/images\"\n",
    "transform = get_transforms(image_size=(224, 224), is_train=True)\n",
    "batch_size = 16\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(data_dir, transform, batch_size)\n",
    "\n",
    "print(f\"[INFO] Number of training batches: {len(train_loader)}\")\n",
    "print(f\"[INFO] Number of testing batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e289114c-2535-42f2-8b92-883e96cdbde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images in train_loader:\n",
    "    print(images.shape)  # Prints the shape of the first batch of images\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46e20c1b-52f3-44e7-8d96-9247ff287816",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     33\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[0;32m---> 34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/projects/particle-detection/.venv/lib/python3.11/site-packages/torch/nn/functional.py:3873\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target, weight):\n\u001b[1;32m   3862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3863\u001b[0m         mse_loss,\n\u001b[1;32m   3864\u001b[0m         (\u001b[38;5;28minput\u001b[39m, target, weight),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3870\u001b[0m         weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m   3871\u001b[0m     )\n\u001b[0;32m-> 3873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[1;32m   3874\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3875\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3876\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3878\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3879\u001b[0m     )\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "num_epochs=1\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# Get transforms\n",
    "transform = get_transforms(image_size=(1024, 1024), is_train=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, test_loader = create_dataloaders(data_dir, transform, batch_size)\n",
    "\n",
    "# Validate batch size\n",
    "if batch_size > len(train_loader.dataset):\n",
    "    print(f\"[WARNING] Batch size ({batch_size}) exceeds dataset size ({len(train_loader.dataset)}). Adjusting batch size.\")\n",
    "    batch_size = len(train_loader.dataset)\n",
    "\n",
    "model = create_autoencoder()\n",
    "model = nn.DataParallel(model)\n",
    "#model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e826ada8-286f-4723-a6b6-1a9398b8622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=128):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 1024 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 512 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 256 -> 128\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(256 * 128 * 128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 128 * 128, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decoder = nn.Linear(latent_dim, 256 * 128 * 128)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 128 -> 256\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 256 -> 512\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, in_channels, kernel_size=4, stride=2, padding=1),  # 512 -> 1024\n",
    "            nn.Sigmoid()  # Output normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + std * epsilon\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        encoded_flat = encoded.view(encoded.size(0), -1)\n",
    "        \n",
    "        # Latent variables\n",
    "        mu = self.fc_mu(encoded_flat)\n",
    "        logvar = self.fc_logvar(encoded_flat)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decode\n",
    "        decoded_flat = self.fc_decoder(z)\n",
    "        decoded = decoded_flat.view(-1, 256, 128, 128)\n",
    "        x_reconstructed = self.decoder(decoded)\n",
    "\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "# Loss function for VAE\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss as the sum of reconstruction loss and KL divergence.\n",
    "    :param recon_x: Reconstructed input.\n",
    "    :param x: Original input.\n",
    "    :param mu: Mean of the latent distribution.\n",
    "    :param logvar: Log variance of the latent distribution.\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "def create_vae():\n",
    "    \"\"\"\n",
    "    Creates and returns a Variational Autoencoder (VAE) instance.\n",
    "    \"\"\"\n",
    "    return VAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8cf8b18-68a1-4662-b823-bf49de78b871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: cpu\n",
      "[INFO] Epoch [1/20], Loss: 5899865.0000\n",
      "[INFO] Epoch [2/20], Loss: 7429085.0000\n",
      "[INFO] Epoch [3/20], Loss: 5658522.5000\n",
      "[INFO] Epoch [4/20], Loss: 5403704.5000\n",
      "[INFO] Epoch [5/20], Loss: 5466216.0000\n",
      "[INFO] Epoch [6/20], Loss: 4563363.0000\n",
      "[INFO] Epoch [7/20], Loss: 4068160.7500\n",
      "[INFO] Epoch [8/20], Loss: 5280898.5000\n",
      "[INFO] Epoch [9/20], Loss: 5148251.0000\n",
      "[INFO] Epoch [10/20], Loss: 4681949.5000\n",
      "[INFO] Epoch [11/20], Loss: 5505831.5000\n",
      "[INFO] Epoch [12/20], Loss: 4605000.0000\n",
      "[INFO] Epoch [13/20], Loss: 4297038.0000\n",
      "[INFO] Epoch [14/20], Loss: 4288450.5000\n",
      "[INFO] Epoch [15/20], Loss: 3508215.5000\n",
      "[INFO] Epoch [16/20], Loss: 4004722.7500\n",
      "[INFO] Epoch [17/20], Loss: 3237115.7500\n",
      "[INFO] Epoch [18/20], Loss: 2965999.0000\n",
      "[INFO] Epoch [19/20], Loss: 2811727.2500\n",
      "[INFO] Epoch [20/20], Loss: 2576518.7500\n",
      "[INFO] Testing complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 20\n",
    "batch_size = 8\n",
    "data_dir = \"/Users/blah_m4/Desktop/nanoparticle/images\"  # Replace with your dataset path\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "# Get transforms\n",
    "transform = get_transforms(image_size=(1024,1024), is_train=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, test_loader = create_dataloaders(data_dir, transform, batch_size)\n",
    "\n",
    "# Validate batch size\n",
    "if batch_size > len(train_loader.dataset):\n",
    "    print(f\"[WARNING] Batch size ({batch_size}) exceeds dataset size ({len(train_loader.dataset)}). Adjusting batch size.\")\n",
    "    batch_size = len(train_loader.dataset)\n",
    "\n",
    "# Create VAE model\n",
    "model = create_vae()\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, mu, logvar = model(batch)\n",
    "        loss = vae_loss(reconstructed, batch, mu, logvar)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"[INFO] Testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5e803a3-7f84-4610-9d15-c9635d3340cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch [1/2], Loss: 2085031.5000, Reconstruction Loss: 1928827.1250, KL Divergence: 156204.3281\n",
      "[INFO] Epoch [2/2], Loss: 2441944.5000, Reconstruction Loss: 2314545.7500, KL Divergence: 127398.7578\n",
      "[INFO] Testing complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    recon_loss_total = 0.0\n",
    "    kl_div_total = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, mu, logvar = model(batch)\n",
    "        \n",
    "        # Compute losses\n",
    "        recon_loss = nn.functional.mse_loss(reconstructed, batch, reduction='sum')  # Reconstruction loss\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL Divergence\n",
    "        loss = recon_loss + kl_div\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        recon_loss_total += recon_loss.item()\n",
    "        kl_div_total += kl_div.item()\n",
    "\n",
    "    print(f\"[INFO] Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "          f\"Reconstruction Loss: {recon_loss_total / len(train_loader):.4f}, \"\n",
    "          f\"KL Divergence: {kl_div_total / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"[INFO] Testing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15a111-802b-4346-b784-c63f2b4925c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
